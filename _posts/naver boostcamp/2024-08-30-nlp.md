---
layout: single
title: "[Week 3]NLP basic"
date: 2024-08-30 20:29:00 +0900
categories: naver-boostcamp
---

이번 주부터 각 도메인 별로 나뉘어져서 개별적인 강의를 수강하게되었습니다. 3~4주차 2주간 NLP 이론에 대해 학습합니다.

## Tokenization

주어진 Text를 Token 단위로 분리하는 방법

- **word-level**
  - OOV(Out Of Vocabulary) 발생 시 `Unknown` 토큰으로 처리
- **character-level**
  - OOV 문제 위험은 없지만, Token 개수가 너무 많아져 장기 기억 약점이 더욱 부곽됨
- **subword-level**
  - 단어를 한번 더 쪼개 subword 단위로 토큰화함
  - (e.g.) 형태소 단위

### BPE(Byte Pair Encoding)

1. 철자 단위의 `token` 목록을 만든다.
2. 가장 빈도수가 높은 `token` pair를 `token` 목록에 추가한다.
3. 최대 Vocab 사이즈에 도달할 때까지 `2`를 반복한다.

위 과정을 통해 BPE Vocab을 만들 수 있다. 새로운 입력 텍스트에 대한 `Tokenization`은 Character 단위로 왼쪽에서부터 vocab의 가장 긴 문자열 토큰으로 매칭한다.

> subword tokenization의 대표적인 예시

### WordPiece

BPE의 변형, 단순히 빈도로 하는 것이 아닌, 언어 모델을 사용하여 likelihood 값을 최대화하는 pair를 탐색 후 vocabulary에 추가한다.

### SentencePiece

wordPiece의 변형, token 앞에 공백이 있는 경우 `_`를 추가로 붙인다. 토큰을 text로 변환 시에도 `_`를 단순히 공백으로 바꾸기만 하면 되어 편리하다.

## Word2Vec

### CBOW(Continuous Bag Of Word)

중심 단어로부터 윈도우 사이즈 만큼 주변 단어들을 사용해 중심단어를 예측하는 task이다.

윈도우 사이즈의 주변단어들의 embedding 값을 `sum`이나 `avg`로 집계한 뒤 output layer로 선형변환 하여 softmax 값을 구해 중심단어를 예측한다.

> 별도의 활성함수가 필요없다!

### Skip-gram

주변 단어 1개로 단어를 예측하는 task이다.

단어 하나로 예측을 하기 때문에 CBOW보다 훨씬 어려운 task여서 모델이 더 많은 것을 학습하게 된다. 또한 하나의 예측단어에 대해 더 많은 데이터를 생성하기 때문에 학습 데이터의 관점에서도 강점이 있다.

> CBOW보다 Skip-gram이 일반적으로 성능이 더 높다고 알려져 있다.

## Truncated Backpropagation Through Time

`RNN` 계열의 `BPTT`는 시퀀스가 길어질수록 각 `timestep`에서의 `activation`등 을 메모리에 전부 보관해야하고, 이는 계산 비용의 증가와 과도한 메모리 요구량으로 이어진다.

이에 대한 방안으로 `Truncated BPTT`가 제시되었다. 시퀀스를 `Chunk` 단위로 나누어 해당 단위 안에서 `forward` 및 `backward`를 수행한다.

`chunk` 단위가 끝난 후 `backward`를 수행한 뒤, 다음 단계로 넘어가야할 `hidden state`만을 남기고 메모리를 비워 효율적인 메모리 관리가 가능하다.

## 고유값 분해로 알아보는 BPTT의 Exploding/Vanishing Gradient

기본적인 `RNN`의 식은 다음과 같다.

$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t + b)$$

이전 `time step`으로 역전파가 될때마다 $W_{hh}^T$가 총 $t-1$번 곱해질 것이다. 이는 정사각 행렬이므로 고유값 분해 `Eigendecomposition`를 적용하여 전개해 볼 수 있다.

$$
\begin{aligned}
W_{hh}^{t-1} &= \left(VDV^{-1}\right)^{t-1} \\
        &= VDV^{-1} \cdot VDV^{-1} \cdot VDV^{-1} \cdot ... \cdot VDV^{-1} \\
        &= VD^{t-1}V^{-1}
\end{aligned}
$$

여기서 D는 대각 행렬(Diagonal Matrix) 이므로 `t-1` 제곱은 각 대각 행렬 원소들에게 그대로 적용되어 `t-1` 제곱을 해야한다. 즉, 각 D의 원소들이 1보다 작은 경우 `Vanishing`이 일어나며 1보다 큰 경우는 `exploding`이 일어나게 되는 것이다.
