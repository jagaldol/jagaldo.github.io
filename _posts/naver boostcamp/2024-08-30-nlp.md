---
layout: single
title: "[Week 3]NLP basic"
date: 2024-08-30 20:29:00 +0900
categories: naver-boostcamp
---

이번 주부터 각 도메인 별로 나뉘어져서 개별적인 강의를 수강하게되었습니다. 3~4주차 2주간 NLP 이론에 대해 학습합니다.

## Tokenization

주어진 Text를 Token 단위로 분리하는 방법

- **word-level**
    - OOV(Out Of Vocabulary) 발생 시 `Unknown` 토큰으로 처리
- **character-level**
    - OOV 문제 위험은 없지만, Token 개수가 너무 많아져 장기 기억 약점이 더욱 부곽됨
- **subword-level**
    - 단어를 한번 더 쪼개 subword 단위로 토큰화함
    - (e.g.) 형태소 단위

### BPE(Byte Pair Encoding)

1. 철자 단위의 `token` 목록을 만든다.
2. 가장 빈도수가 높은 `token` pair를 `token` 목록에 추가한다.
3. 최대 Vocab 사이즈에 도달할 때까지 `2`를 반복한다.

위 과정을 통해 BPE Vocab을 만들 수 있다. 새로운 입력 텍스트에 대한 `Tokenization`은 Character 단위로 왼쪽에서부터 vocab의 가장 긴 문자열 토큰으로 매칭한다.

> subword tokenization의 대표적인 예시

### WordPiece

BPE의 변형, 단순히 빈도로 하는 것이 아닌, 언어 모델을 사용하여 likelihood 값을 최대화하는 pair를 탐색 후 vocabulary에 추가한다.

### SentencePiece

wordPiece의 변형, token 앞에 공백이 있는 경우 `_`를 추가로 붙인다. 토큰을 text로 변환 시에도 `_`를 단순히 공백으로 바꾸기만 하면 되어 편리하다.

## Word2Vec

### CBOW(Continuous Bag Of Word)

중심 단어로부터 윈도우 사이즈 만큼 주변 단어들을 사용해 중심단어를 예측하는 task이다.

윈도우 사이즈의 주변단어들의 embedding 값을 `sum`이나 `avg`로 집계한 뒤 output layer로 선형변환 하여 softmax 값을 구해 중심단어를 예측한다.

> 별도의 활성함수가 필요없다!

### Skip-gram

주변 단어 1개로 단어를 예측하는 task이다. 

단어 하나로 예측을 하기 때문에 CBOW보다 훨씬 어려운 task여서 모델이 더 많은 것을 학습하게 된다. 또한 하나의 예측단어에 대해 더 많은 데이터를 생성하기 때문에 학습 데이터의 관점에서도 강점이 있다.

> CBOW보다 Skip-gram이 일반적으로 성능이 더 높다고 알려져 있다.